{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db320e8b-b83b-40ad-8d1e-b57d30ff55d9",
   "metadata": {},
   "source": [
    "# This is a notebook for testing diff models and architectures \n",
    "checked into git because im too lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68466e17-08ee-4252-af00-b67ec294764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead9fc04-c8fb-4370-9d0b-bc2e6075d90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "osu! path: None/../Local/osu!\n"
     ]
    }
   ],
   "source": [
    "from models.vae.vae_t import OsuReplayTVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f15e4c0-8409-4269-bf27-3240fa098a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplayTVAE initialized on cuda\n",
      "decoder parameters: 2806210\n",
      "encoder parameters: 2621952\n",
      "Total parameters: 5428162\n",
      "OsuReplayTVAE loaded from replaytvae_morerecent.pt\n"
     ]
    }
   ],
   "source": [
    "model = OsuReplayTVAE.load(\"replaytvae_morerecent.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f7f9b03-ebd6-4017-8446-614dd572d30f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# test and benchmark regular LSTM vs cudnn lstm\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# test and benchmark regular LSTM vs cudnn lstm\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.modules.rnn import RNNCellBase\n",
    "\n",
    "class LSTMCell(RNNCellBase):\n",
    "    __doc__ += nn.LSTMCell.__doc__\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        bias: bool = True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ) -> None:\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__(input_size, hidden_size, bias, num_chunks=4, **factory_kwargs)\n",
    "\n",
    "    def forward(\n",
    "        self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = None\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        if input.dim() not in (1, 2):\n",
    "            raise ValueError(\n",
    "                f\"LSTMCell: Expected input to be 1D or 2D, got {input.dim()}D instead\"\n",
    "            )\n",
    "        if hx is not None:\n",
    "            for idx, value in enumerate(hx):\n",
    "                if value.dim() not in (1, 2):\n",
    "                    raise ValueError(\n",
    "                        f\"LSTMCell: Expected hx[{idx}] to be 1D or 2D, got {value.dim()}D instead\"\n",
    "                    )\n",
    "        is_batched = input.dim() == 2\n",
    "        if not is_batched:\n",
    "            input = input.unsqueeze(0)\n",
    "\n",
    "        if hx is None:\n",
    "            zeros = torch.zeros(\n",
    "                input.size(0), self.hidden_size, dtype=input.dtype, device=input.device\n",
    "            )\n",
    "            hx = (zeros, zeros)\n",
    "        else:\n",
    "            hx = (hx[0].unsqueeze(0), hx[1].unsqueeze(0)) if not is_batched else hx\n",
    "\n",
    "        ret = self.lstm_cell(input, hx[0], hx[1])\n",
    "\n",
    "        if not is_batched:\n",
    "            ret = (ret[0].squeeze(0), ret[1].squeeze(0))\n",
    "        return ret\n",
    "\n",
    "    def lstm_cell(self, x, hx, cx):\n",
    "        x = x.view(-1, x.size(1))\n",
    "\n",
    "        gates = F.linear(x, self.weight_ih, self.bias_ih) + F.linear(\n",
    "            hx, self.weight_hh, self.bias_hh\n",
    "        )\n",
    "\n",
    "        i_gate, f_gate, g_gate, o_gate = gates.chunk(4, 1)\n",
    "\n",
    "        i_gate = i_gate.sigmoid()\n",
    "        f_gate = f_gate.sigmoid()\n",
    "        g_gate = g_gate.tanh()\n",
    "        o_gate = o_gate.sigmoid()\n",
    "\n",
    "        cy = cx * f_gate + i_gate * g_gate\n",
    "\n",
    "        hy = o_gate * cy.tanh()\n",
    "\n",
    "        return hy, cy\n",
    "\n",
    "\n",
    "# copy LSTM\n",
    "class LSTMBase(nn.RNNBase):\n",
    "    \"\"\"A Base module for LSTM. Inheriting from LSTMBase enables compatibility with torch.compile.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        return super().__init__(\"LSTM\", *args, **kwargs)\n",
    "\n",
    "\n",
    "for attr in nn.LSTM.__dict__:\n",
    "    if attr != \"__init__\":\n",
    "        setattr(LSTMBase, attr, getattr(nn.LSTM, attr))\n",
    "\n",
    "\n",
    "class LSTM(LSTMBase):\n",
    "    __doc__ += nn.LSTM.__doc__\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int = 1,\n",
    "        batch_first: bool = True,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "        bidirectional: float = False,\n",
    "        proj_size: int = 0,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ) -> None:\n",
    "\n",
    "        if bidirectional is True:\n",
    "            raise NotImplementedError(\n",
    "                \"Bidirectional LSTMs are not supported yet in this implementation.\"\n",
    "            )\n",
    "\n",
    "        super().__init__(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bias=bias,\n",
    "            batch_first=batch_first,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "            proj_size=proj_size,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _lstm_cell(x, hx, cx, weight_ih, bias_ih, weight_hh, bias_hh):\n",
    "\n",
    "        gates = F.linear(x, weight_ih, bias_ih) + F.linear(hx, weight_hh, bias_hh)\n",
    "\n",
    "        i_gate, f_gate, g_gate, o_gate = gates.chunk(4, 1)\n",
    "\n",
    "        i_gate = i_gate.sigmoid()\n",
    "        f_gate = f_gate.sigmoid()\n",
    "        g_gate = g_gate.tanh()\n",
    "        o_gate = o_gate.sigmoid()\n",
    "\n",
    "        cy = cx * f_gate + i_gate * g_gate\n",
    "\n",
    "        hy = o_gate * cy.tanh()\n",
    "\n",
    "        return hy, cy\n",
    "\n",
    "    def _lstm(self, x, hx):\n",
    "\n",
    "        h_t, c_t = hx\n",
    "        h_t, c_t = h_t.unbind(0), c_t.unbind(0)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        weight_ihs = []\n",
    "        weight_hhs = []\n",
    "        bias_ihs = []\n",
    "        bias_hhs = []\n",
    "        for weights in self._all_weights:\n",
    "            # Retrieve weights\n",
    "            weight_ihs.append(getattr(self, weights[0]))\n",
    "            weight_hhs.append(getattr(self, weights[1]))\n",
    "            if self.bias:\n",
    "                bias_ihs.append(getattr(self, weights[2]))\n",
    "                bias_hhs.append(getattr(self, weights[3]))\n",
    "            else:\n",
    "                bias_ihs.append(None)\n",
    "                bias_hhs.append(None)\n",
    "\n",
    "        for x_t in x.unbind(int(self.batch_first)):\n",
    "            h_t_out = []\n",
    "            c_t_out = []\n",
    "\n",
    "            for layer, (\n",
    "                weight_ih,\n",
    "                bias_ih,\n",
    "                weight_hh,\n",
    "                bias_hh,\n",
    "                _h_t,\n",
    "                _c_t,\n",
    "            ) in enumerate(zip(weight_ihs, bias_ihs, weight_hhs, bias_hhs, h_t, c_t)):\n",
    "                # Run cell\n",
    "                _h_t, _c_t = self._lstm_cell(\n",
    "                    x_t, _h_t, _c_t, weight_ih, bias_ih, weight_hh, bias_hh\n",
    "                )\n",
    "                h_t_out.append(_h_t)\n",
    "                c_t_out.append(_c_t)\n",
    "\n",
    "                # Apply dropout if in training mode\n",
    "                if layer < self.num_layers - 1 and self.dropout:\n",
    "                    x_t = F.dropout(_h_t, p=self.dropout, training=self.training)\n",
    "                else:  # No dropout after the last layer\n",
    "                    x_t = _h_t\n",
    "            h_t = h_t_out\n",
    "            c_t = c_t_out\n",
    "            outputs.append(x_t)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=int(self.batch_first))\n",
    "\n",
    "        return outputs, (torch.stack(h_t_out, 0), torch.stack(c_t_out, 0))\n",
    "\n",
    "    def forward(self, input, hx=None):  # noqa: F811\n",
    "        real_hidden_size = self.proj_size if self.proj_size > 0 else self.hidden_size\n",
    "        if input.dim() != 3:\n",
    "            raise ValueError(\n",
    "                f\"LSTM: Expected input to be 3D, got {input.dim()}D instead\"\n",
    "            )\n",
    "        max_batch_size = input.size(0) if self.batch_first else input.size(1)\n",
    "        if hx is None:\n",
    "            h_zeros = torch.zeros(\n",
    "                self.num_layers,\n",
    "                max_batch_size,\n",
    "                real_hidden_size,\n",
    "                dtype=input.dtype,\n",
    "                device=input.device,\n",
    "            )\n",
    "            c_zeros = torch.zeros(\n",
    "                self.num_layers,\n",
    "                max_batch_size,\n",
    "                self.hidden_size,\n",
    "                dtype=input.dtype,\n",
    "                device=input.device,\n",
    "            )\n",
    "            hx = (h_zeros, c_zeros)\n",
    "        return self._lstm(input, hx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "239efc00-d706-4004-9073-321cc12b3311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "import osu.dataset as dataset\n",
    "\n",
    "from models.annealer import Annealer\n",
    "from models.base import OsuModel\n",
    "from models.model_utils import TransformerArgs\n",
    "from models.vae.encoder_t import ReplayEncoderT\n",
    "\n",
    "\n",
    "\n",
    "class ReplayDecoder_Lstm(nn.Module):\n",
    "    def __init__(self, input_size, latent_dim=32, past_frames=0, future_frames=0):\n",
    "        super().__init__()\n",
    "        self.past_frames = past_frames\n",
    "        self.future_frames = future_frames\n",
    "        self.window_size = past_frames + 1 + future_frames\n",
    "\n",
    "        combined_size = (input_size) + latent_dim\n",
    "\n",
    "        # Symmetric layers to encoder (not really almost)\n",
    "        self.lstm = nn.LSTM(combined_size, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
    "\n",
    "        self.dense1 = nn.Linear(256, 128)\n",
    "        self.dense2 = nn.Linear(128, 64)\n",
    "\n",
    "        self.output_layer = nn.Linear(64, 2)  # x, y positions\n",
    "\n",
    "    def forward(self, beatmap_features, latent_code):\n",
    "        batch_size, seq_len, _ = beatmap_features.shape\n",
    "\n",
    "        # beatmap_features is already windowed\n",
    "        # Expand latent code to sequence length\n",
    "        latent_expanded = latent_code.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\n",
    "        # Combine windowed features with latent code\n",
    "        x = torch.cat([beatmap_features, latent_expanded], dim=-1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        features = F.relu(self.dense1(lstm_out))\n",
    "        features = F.relu(self.dense2(features))\n",
    "\n",
    "        positions = self.output_layer(features)\n",
    "\n",
    "        return positions\n",
    "\n",
    "\n",
    "# switch from the RNN based VAE to a transformer based one\n",
    "# TODO! save hyperparam dict\n",
    "class OsuReplayTVAE_Lstm(OsuModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        annealer: Annealer = None,\n",
    "        batch_size=64,\n",
    "        device=None,\n",
    "        latent_dim=64,\n",
    "        transformer_args: TransformerArgs = None,\n",
    "        noise_std=0.0, \n",
    "        frame_window=(40, 90),\n",
    "        compile: bool = True\n",
    "    ):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.transformer_args = transformer_args or TransformerArgs()\n",
    "        self.past_frames = frame_window[0]\n",
    "        self.future_frames = frame_window[1]\n",
    "        self.noise_std = noise_std\n",
    "        self.annealer = annealer or Annealer(\n",
    "            total_steps=10, range=(0, 0.3), cyclical=True, stay_max_steps=5\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "            compile=compile\n",
    "        )\n",
    "\n",
    "    def _initialize_models(self, **kwargs):\n",
    "        self.encoder = ReplayEncoderT(\n",
    "            input_size=self.input_size,\n",
    "            latent_dim=self.latent_dim,\n",
    "            transformer_args=self.transformer_args,\n",
    "            noise_std=self.noise_std,\n",
    "            past_frames=self.past_frames,\n",
    "            future_frames=self.future_frames,\n",
    "        )\n",
    "        self.decoder = ReplayDecoder_Lstm(\n",
    "            input_size=self.transformer_args.embed_dim,\n",
    "            latent_dim=self.latent_dim,\n",
    "            past_frames=self.past_frames,\n",
    "            future_frames=self.future_frames,\n",
    "        )\n",
    "\n",
    "    def _initialize_optimizers(self):\n",
    "        params = list(self.encoder.parameters()) + list(self.decoder.parameters())\n",
    "        self.optimizer = optim.AdamW(\n",
    "            params, lr=0.001, betas=(0.9, 0.999), weight_decay=0.001\n",
    "        )\n",
    "\n",
    "\n",
    "    def _train_epoch(self, epoch, total_epochs, **kwargs):\n",
    "        epoch_total_loss = 0\n",
    "        epoch_recon_loss = 0\n",
    "        epoch_kl_loss = 0\n",
    "\n",
    "        for i, (batch_x, batch_y_pos) in enumerate(\n",
    "            tqdm.tqdm(\n",
    "                self.train_loader,\n",
    "                disable=True,\n",
    "                position=1,\n",
    "                desc=f\"Epoch {epoch + 1}/{total_epochs} (Beta: {self.annealer.current()})\",\n",
    "            )\n",
    "        ):\n",
    "            self._set_custom_train_status(f\"Batch {i}/{len(self.train_loader)}\")\n",
    "            batch_x = batch_x.to(self.device)             # (B, T, features)\n",
    "            batch_y_pos = batch_y_pos.to(self.device)     # (B, T, pos)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            reconstructed, mu, logvar = self.forward(batch_x, batch_y_pos)\n",
    "\n",
    "            # Compute loss\n",
    "            total_loss, recon_loss, kl_loss = self.loss_function(\n",
    "                reconstructed, batch_y_pos, mu, logvar\n",
    "            )\n",
    "\n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(self.encoder.parameters()) + list(self.decoder.parameters()),\n",
    "                max_norm=1.0,\n",
    "            )\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_total_loss += total_loss.item()\n",
    "            epoch_recon_loss += recon_loss.item()\n",
    "            epoch_kl_loss += kl_loss.item()\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_total_loss = epoch_total_loss / len(self.train_loader)\n",
    "        avg_recon_loss = epoch_recon_loss / len(self.train_loader)\n",
    "        avg_kl_loss = epoch_kl_loss / len(self.train_loader)\n",
    "\n",
    "        # Step the annealer\n",
    "        self.annealer.step()\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": avg_total_loss,\n",
    "            \"recon_loss\": avg_recon_loss,\n",
    "            \"kl_loss\": avg_kl_loss,\n",
    "        }\n",
    "\n",
    "    # allow backpropogatino through sampling\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, beatmap_features, positions):\n",
    "        embeddings, mu, logvar = self.encoder(beatmap_features)\n",
    "\n",
    "        # Sample latent code\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        reconstructed = self.decoder(embeddings, z)\n",
    "\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "    # recon + kl term\n",
    "    def loss_function(self, reconstructed, original, mu, logvar):\n",
    "        # TODO! am i supposed to avg this? probably?\n",
    "        recon_loss = F.mse_loss(reconstructed, original, reduction=\"sum\")\n",
    "        recon_loss /= original.shape[0]\n",
    "\n",
    "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kld /= original.shape[0]\n",
    "\n",
    "        total_loss = recon_loss + self.annealer(kld)\n",
    "\n",
    "        return total_loss, recon_loss, kld\n",
    "\n",
    "    def _get_state_dict(self):\n",
    "        return {\n",
    "            \"encoder\": self.encoder.state_dict(),\n",
    "            \"decoder\": self.decoder.state_dict(),\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"transformer_args\": self.transformer_args.to_dict(),\n",
    "            \"noise_std\": self.noise_std,\n",
    "            \"input_size\": self.input_size,\n",
    "            \"past_frames\": self.past_frames,\n",
    "            \"future_frames\": self.future_frames,\n",
    "        }\n",
    "\n",
    "    def _load_state_dict(self, checkpoint):\n",
    "        self.encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "        self.decoder.load_state_dict(checkpoint[\"decoder\"])\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str, device: Optional[torch.device] = None, **kwargs):\n",
    "        device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        \n",
    "        # Load transformer configuration from checkpoint\n",
    "        transformer_args = TransformerArgs.from_dict(checkpoint[\"transformer_args\"])\n",
    "        \n",
    "        vae_args = {\n",
    "            \"latent_dim\": checkpoint.get(\"latent_dim\", 64),\n",
    "            \"transformer_args\": transformer_args,\n",
    "            \"noise_std\": checkpoint.get(\"noise_std\", 0.0),\n",
    "            \"frame_window\": (checkpoint.get(\"past_frames\", 40), checkpoint.get(\"future_frames\", 90)),\n",
    "        }\n",
    "\n",
    "        instance = cls(device=device, **kwargs, **vae_args)\n",
    "\n",
    "        instance._load_state_dict(checkpoint)\n",
    "        instance._set_eval_mode()\n",
    "\n",
    "        print(f\"{cls.__name__} loaded from {path}\")\n",
    "        return instance\n",
    "\n",
    "    def generate(self, beatmap_data, num_samples=1):\n",
    "        self._set_eval_mode()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            beatmap_tensor = torch.FloatTensor(beatmap_data).to(self.device)\n",
    "\n",
    "            batch_size = beatmap_tensor.shape[0]\n",
    "\n",
    "            # sample from prior distribution\n",
    "            z = torch.randn(batch_size, self.latent_dim, device=self.device)\n",
    "            embeddings, mu, logvar = self.encoder(beatmap_tensor)\n",
    "            # z = self.reparameterize(mu, logvar)\n",
    "\n",
    "            pos = self.decoder(embeddings, z)\n",
    "\n",
    "        self._set_train_mode()\n",
    "\n",
    "        return pos.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b91adb4-d92f-4294-b083-e0b87050587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplayVAE initialized on cuda\n",
      "decoder parameters: 452978\n",
      "encoder parameters: 646912\n",
      "Total parameters: 1099890\n",
      "OsuReplayVAE loaded from replayvae_most_recent.pt\n"
     ]
    }
   ],
   "source": [
    "# model = OsuReplayTVAE_Lstm.load(\"replaytvae_lstm_most_recent.pt\")\n",
    "from models.vae.vae import OsuReplayVAE\n",
    "model = OsuReplayVAE.load(\"replayvae_most_recent.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5583978-5d5b-4fd7-92b4-b4036b552bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2048, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test training results\n",
    "from osu.rulesets.mods import Mods\n",
    "import osu.rulesets.beatmap as bm\n",
    "import osu.dataset as dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "test_name = '1hope'\n",
    "test_mods = Mods.NONE\n",
    "test_map_path = f'assets/{test_name}_map.osu'\n",
    "test_song = f'assets/{test_name}_song.mp3'\n",
    "\n",
    "test_map = bm.load(test_map_path)\n",
    "test_map.apply_mods(test_mods)\n",
    "\n",
    "data = dataset.input_data(test_map)\n",
    "data = np.reshape(data.values, (-1, dataset.BATCH_LENGTH, len(dataset.INPUT_FEATURES)))\n",
    "data = torch.FloatTensor(data)#[:1, :, :]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddd772b4-96f4-4342-8a73-cf987d3cffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated replay data shape: (10240, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03608687,  0.04201587,  0.        ,  0.        ],\n",
       "       [-0.06219175,  0.03570696,  0.        ,  0.        ],\n",
       "       [-0.08341248,  0.04224943,  0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.07666025,  0.09332845,  0.        ,  0.        ],\n",
       "       [-0.07671017,  0.09339259,  0.        ,  0.        ],\n",
       "       [-0.0767544 ,  0.09344955,  0.        ,  0.        ]],\n",
       "      shape=(10240, 4), dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_data = model.generate(data)\n",
    "\n",
    "import os\n",
    "\n",
    "replay_data = np.concatenate(replay_data)\n",
    "replay_data = np.pad(replay_data, ((0, 0), (0, 2)), mode='constant', constant_values=0)\n",
    "if not os.path.exists('.generated'):\n",
    "    os.makedirs('.generated')\n",
    "\n",
    "print(f\"Generated replay data shape: {replay_data.shape}\")\n",
    "\n",
    "replay_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0003d66-21df-4c3c-9166-1d101732f31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm.c:8772:(snd_pcm_recover) underrun occurred\n"
     ]
    }
   ],
   "source": [
    "import osu.preview.preview as preview\n",
    "\n",
    "preview.preview_replay_raw(replay_data, test_map_path, test_mods, test_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476a43a-93fa-40e8-bb76-ae62b25c794c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204bc08-ca13-4365-b67c-7862cc8fa751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
