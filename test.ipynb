{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db320e8b-b83b-40ad-8d1e-b57d30ff55d9",
   "metadata": {},
   "source": [
    "# This is a notebook for testing diff models and architectures \n",
    "checked into git because im too lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68466e17-08ee-4252-af00-b67ec294764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead9fc04-c8fb-4370-9d0b-bc2e6075d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vae.vae_t import OsuReplayTVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f15e4c0-8409-4269-bf27-3240fa098a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplayTVAE initialized on cuda\n",
      "decoder parameters: 2806210\n",
      "encoder parameters: 2621952\n",
      "Total parameters: 5428162\n",
      "OsuReplayTVAE loaded from replaytvae_morerecent.pt\n"
     ]
    }
   ],
   "source": [
    "model = OsuReplayTVAE.load(\"replaytvae_morerecent.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "239efc00-d706-4004-9073-321cc12b3311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "import osu.dataset as dataset\n",
    "\n",
    "from models.annealer import Annealer\n",
    "from models.base import OsuModel\n",
    "from models.model_utils import TransformerArgs\n",
    "from models.vae.encoder import ReplayEncoder\n",
    "from models.standard_encoder_t import MapEncoder\n",
    "\n",
    "class ReplayEncoder_Lstm(nn.Module):\n",
    "    def __init__(self, input_size, latent_dim=32, noise_std=0.0, past_frames=0, future_frames=0):\n",
    "        super().__init__()\n",
    "        self.past_frames = past_frames\n",
    "        self.future_frames = future_frames\n",
    "        self.window_size = past_frames + 1 + future_frames  # +1 for current frame\n",
    "\n",
    "        # Windowed beatmap features + cursor positions\n",
    "        # TODO! testing unconditional vae\n",
    "        combined_size = (input_size * self.window_size) + 2\n",
    "\n",
    "        self.lstm = nn.LSTM(combined_size, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
    "\n",
    "        self.dense1 = nn.Linear(256, 128)\n",
    "        self.noise_std = noise_std\n",
    "        self.dense2 = nn.Linear(128, 64)\n",
    "\n",
    "        # Output layers for reparameterization trick\n",
    "        self.mu_layer = nn.Linear(64, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(64, latent_dim)\n",
    "\n",
    "    def forward(self, beatmap_features, positions):\n",
    "        # beatmap_features is now already windowed\n",
    "        # gaussian noise during training\n",
    "        if self.training and self.noise_std > 0:\n",
    "            noise = torch.randn_like(positions) * self.noise_std\n",
    "            positions = positions + noise\n",
    "\n",
    "        # Combine windowed beatmap features with positions\n",
    "        # (embeddings in this case)\n",
    "        x = torch.cat([beatmap_features, positions], dim=-1)\n",
    "        # x = beatmap_features\n",
    "\n",
    "        # Encode sequence\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "\n",
    "        h = h_n[-1]\n",
    "\n",
    "        h = F.relu(self.dense1(h))\n",
    "\n",
    "        h = F.relu(self.dense2(h))\n",
    "\n",
    "        # Output mean and log variance for reparameterization trick\n",
    "        mu = self.mu_layer(h)\n",
    "        logvar = self.logvar_layer(h)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "class ReplayDecoder_Lstm(nn.Module):\n",
    "    \"\"\"Decode latent code + beatmap features to cursor positions\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, latent_dim=48, past_frames=0, future_frames=0):\n",
    "        super().__init__()\n",
    "        self.past_frames = past_frames\n",
    "        self.future_frames = future_frames\n",
    "        self.window_size = past_frames + 1 + future_frames\n",
    "\n",
    "        combined_size = (input_size) + latent_dim\n",
    "\n",
    "        # Symmetric layers to encoder (not really almost)\n",
    "        self.lstm = nn.LSTM(combined_size, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
    "\n",
    "        self.dense1 = nn.Linear(256, 128)\n",
    "        self.dense2 = nn.Linear(128, 64)\n",
    "\n",
    "        self.output_layer = nn.Linear(64, 2)  # x, y positions\n",
    "\n",
    "    def forward(self, beatmap_features, latent_code):\n",
    "        batch_size, seq_len, _ = beatmap_features.shape\n",
    "\n",
    "        # beatmap_features is already windowed\n",
    "        # Expand latent code to sequence length\n",
    "        latent_expanded = latent_code.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\n",
    "        # Combine windowed features with latent code\n",
    "        x = torch.cat([beatmap_features, latent_expanded], dim=-1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        features = F.relu(self.dense1(lstm_out))\n",
    "        features = F.relu(self.dense2(features))\n",
    "\n",
    "        positions = self.output_layer(features)\n",
    "\n",
    "        return positions\n",
    "\n",
    "\n",
    "# switch from the RNN based VAE to a transformer based one\n",
    "# TODO! save hyperparam dict\n",
    "class OsuReplayTVAE_Lstm(OsuModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        annealer: Annealer = None,\n",
    "        batch_size=64,\n",
    "        device=None,\n",
    "        latent_dim=64,\n",
    "        transformer_args: TransformerArgs = None,\n",
    "        noise_std=0.0,\n",
    "        frame_window=(400, 900),\n",
    "        compile: bool = True\n",
    "    ):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.transformer_args = transformer_args or TransformerArgs()\n",
    "        self.past_frames = frame_window[0]\n",
    "        self.future_frames = frame_window[1]\n",
    "        self.noise_std = noise_std\n",
    "        self.annealer = annealer or Annealer(\n",
    "            total_steps=10, range=(0, 0.3), cyclical=True, stay_max_steps=5\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "            compile=compile\n",
    "        )\n",
    "\n",
    "    def _initialize_models(self, **kwargs):\n",
    "        self.encoder = ReplayEncoder_Lstm(\n",
    "            input_size=self.transformer_args.embed_dim,\n",
    "            latent_dim=self.latent_dim,\n",
    "            noise_std=self.noise_std,\n",
    "            # map encoder already encodes information in a window\n",
    "            past_frames=0,\n",
    "            future_frames=0,\n",
    "        )\n",
    "\n",
    "        self.map_encoder = MapEncoder(\n",
    "            input_size=len(dataset.INPUT_FEATURES),\n",
    "            transformer_args=self.transformer_args,\n",
    "            future_frames=self.future_frames,\n",
    "            past_frames=self.past_frames\n",
    "        )\n",
    "\n",
    "        self.decoder = ReplayDecoder_Lstm(\n",
    "            input_size=self.transformer_args.embed_dim,\n",
    "            latent_dim=self.latent_dim,\n",
    "            past_frames=self.past_frames,\n",
    "            future_frames=self.future_frames,\n",
    "        )\n",
    "\n",
    "    def _initialize_optimizers(self):\n",
    "        params = list(self.encoder.parameters()) + list(self.decoder.parameters()) + list(self.map_encoder.parameters())\n",
    "        self.optimizer = optim.AdamW(\n",
    "            params, lr=0.001, betas=(0.9, 0.999), weight_decay=0.001\n",
    "        )\n",
    "\n",
    "\n",
    "    def _train_epoch(self, epoch, total_epochs, **kwargs):\n",
    "        epoch_total_loss = 0\n",
    "        epoch_recon_loss = 0\n",
    "        epoch_kl_loss = 0\n",
    "\n",
    "        for i, (batch_x, batch_y_pos) in enumerate(\n",
    "            tqdm.tqdm(\n",
    "                self.train_loader,\n",
    "                disable=True,\n",
    "                position=1,\n",
    "                desc=f\"Epoch {epoch + 1}/{total_epochs} (Beta: {self.annealer.current()})\",\n",
    "            )\n",
    "        ):\n",
    "            self._set_custom_train_status(f\"Batch {i}/{len(self.train_loader)}\")\n",
    "            batch_x = batch_x.to(self.device)             # (B, T, features)\n",
    "            batch_y_pos = batch_y_pos.to(self.device)     # (B, T, pos)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            reconstructed, mu, logvar = self.forward(batch_x, batch_y_pos)\n",
    "\n",
    "            # Compute loss\n",
    "            total_loss, recon_loss, kl_loss = self.loss_function(\n",
    "                reconstructed, batch_y_pos, mu, logvar\n",
    "            )\n",
    "\n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(self.encoder.parameters()) + list(self.decoder.parameters()),\n",
    "                max_norm=1.0,\n",
    "            )\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_total_loss += total_loss.item()\n",
    "            epoch_recon_loss += recon_loss.item()\n",
    "            epoch_kl_loss += kl_loss.item()\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_total_loss = epoch_total_loss / len(self.train_loader)\n",
    "        avg_recon_loss = epoch_recon_loss / len(self.train_loader)\n",
    "        avg_kl_loss = epoch_kl_loss / len(self.train_loader)\n",
    "\n",
    "        # Step the annealer\n",
    "        self.annealer.step()\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": avg_total_loss,\n",
    "            \"recon_loss\": avg_recon_loss,\n",
    "            \"kl_loss\": avg_kl_loss,\n",
    "        }\n",
    "\n",
    "    # allow backpropogatino through sampling\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, beatmap_features, positions):\n",
    "        embeddings = self.map_encoder(beatmap_features)\n",
    "\n",
    "        mu, logvar = self.encoder(embeddings, positions)\n",
    "\n",
    "        # Sample latent code\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        reconstructed = self.decoder(embeddings, z)\n",
    "\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "    # recon + kl term\n",
    "    def loss_function(self, reconstructed, original, mu, logvar):\n",
    "        # TODO! am i supposed to avg this? probably?\n",
    "        recon_loss = F.mse_loss(reconstructed, original, reduction=\"sum\")\n",
    "        # recon_loss /= original.shape[0]\n",
    "\n",
    "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kld /= original.shape[0]\n",
    "\n",
    "        total_loss = recon_loss + self.annealer(kld)\n",
    "\n",
    "        return total_loss, recon_loss, kld\n",
    "\n",
    "    def _get_state_dict(self):\n",
    "        return {\n",
    "            \"encoder\": self.encoder.state_dict(),\n",
    "            \"decoder\": self.decoder.state_dict(),\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"transformer_args\": self.transformer_args.to_dict(),\n",
    "            \"noise_std\": self.noise_std,\n",
    "            \"input_size\": self.input_size,\n",
    "            \"past_frames\": self.past_frames,\n",
    "            \"future_frames\": self.future_frames,\n",
    "        }\n",
    "\n",
    "    def _load_state_dict(self, checkpoint):\n",
    "        self.encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "        self.decoder.load_state_dict(checkpoint[\"decoder\"])\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str, device: Optional[torch.device] = None, **kwargs):\n",
    "        device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "        # Load transformer configuration from checkpoint\n",
    "        transformer_args = TransformerArgs.from_dict(checkpoint[\"transformer_args\"])\n",
    "\n",
    "        vae_args = {\n",
    "            \"latent_dim\": checkpoint.get(\"latent_dim\", 64),\n",
    "            \"transformer_args\": transformer_args,\n",
    "            \"noise_std\": checkpoint.get(\"noise_std\", 0.0),\n",
    "            \"frame_window\": (checkpoint.get(\"past_frames\", 40), checkpoint.get(\"future_frames\", 90)),\n",
    "        }\n",
    "\n",
    "        instance = cls(device=device, **kwargs, **vae_args)\n",
    "\n",
    "        instance._load_state_dict(checkpoint)\n",
    "        instance._set_eval_mode()\n",
    "\n",
    "        print(f\"{cls.__name__} loaded from {path}\")\n",
    "        return instance\n",
    "\n",
    "    def generate(self, beatmap_data, num_samples=1):\n",
    "        self._set_eval_mode()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            beatmap_tensor = torch.FloatTensor(beatmap_data).to(self.device)\n",
    "\n",
    "            batch_size = beatmap_tensor.shape[0]\n",
    "\n",
    "            # sample from prior distribution\n",
    "            z = torch.randn(batch_size, self.latent_dim, device=self.device)\n",
    "            embeddings = self.map_encoder(beatmap_tensor)\n",
    "            # embeddings, mu, logvar = self.encoder(embeddings)\n",
    "            # z = self.reparameterize(mu, logvar)\n",
    "\n",
    "            pos = self.decoder(embeddings, z)\n",
    "\n",
    "        self._set_train_mode()\n",
    "\n",
    "        return pos.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b91adb4-d92f-4294-b083-e0b87050587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplayTVAE_Lstm initialized on cuda\n",
      "decoder parameters: 1028418\n",
      "encoder parameters: 973120\n",
      "map_encoder parameters: 2572416\n",
      "Total parameters: 4573954\n",
      "OsuReplayTVAE_Lstm loaded from replaytvae_lstm_most_recent.pt\n"
     ]
    }
   ],
   "source": [
    "model = OsuReplayTVAE_Lstm.load(\"replaytvae_lstm_most_recent.pt\")\n",
    "# from models.vae.vae import OsuReplayVAE\n",
    "# model = OsuReplayVAE.load(\"replayvae_most_recent.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5583978-5d5b-4fd7-92b4-b4036b552bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Turning 1HOPE SNIPER into time series data: 100%|████████████████████████████████| 1/1 [00:00<00:00,  4.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2048, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test training results\n",
    "from osu.rulesets.mods import Mods\n",
    "import osu.rulesets.beatmap as bm\n",
    "import osu.dataset as dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "test_name = '1hope'\n",
    "test_mods = Mods.NONE\n",
    "test_map_path = f'assets/{test_name}_map.osu'\n",
    "test_song = f'assets/{test_name}_song.mp3'\n",
    "\n",
    "test_map = bm.load(test_map_path)\n",
    "test_map.apply_mods(test_mods)\n",
    "\n",
    "data = dataset.input_data(test_map)\n",
    "data = np.reshape(data.values, (-1, dataset.BATCH_LENGTH, len(dataset.INPUT_FEATURES)))\n",
    "data = torch.FloatTensor(data)#[:1, :, :]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddd772b4-96f4-4342-8a73-cf987d3cffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niooi/projects/osu/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W0901 12:41:11.354000 5318 torch/_inductor/utils.py:1250] [1/0_1] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated replay data shape: (10240, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.06962939, -0.06804687,  0.        ,  0.        ],\n",
       "       [ 0.13007933, -0.00253613,  0.        ,  0.        ],\n",
       "       [ 0.2266632 , -0.16867389,  0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.18648267, -0.15628761,  0.        ,  0.        ],\n",
       "       [ 0.25100222, -0.10195369,  0.        ,  0.        ],\n",
       "       [ 0.31378463,  0.03294045,  0.        ,  0.        ]],\n",
       "      shape=(10240, 4), dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_data = model.generate(data)\n",
    "\n",
    "import os\n",
    "\n",
    "replay_data = np.concatenate(replay_data)\n",
    "replay_data = np.pad(replay_data, ((0, 0), (0, 2)), mode='constant', constant_values=0)\n",
    "if not os.path.exists('.generated'):\n",
    "    os.makedirs('.generated')\n",
    "\n",
    "print(f\"Generated replay data shape: {replay_data.shape}\")\n",
    "\n",
    "replay_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0003d66-21df-4c3c-9166-1d101732f31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niooi/projects/osu/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import osu.preview.preview as preview\n",
    "\n",
    "preview.preview_replay_raw(replay_data, test_map_path, test_mods, test_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476a43a-93fa-40e8-bb76-ae62b25c794c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204bc08-ca13-4365-b67c-7862cc8fa751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
