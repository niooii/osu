{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db320e8b-b83b-40ad-8d1e-b57d30ff55d9",
   "metadata": {},
   "source": [
    "# This is a notebook for testing diff models and architectures \n",
    "checked into git because im too lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68466e17-08ee-4252-af00-b67ec294764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead9fc04-c8fb-4370-9d0b-bc2e6075d90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "osu! path: None/../Local/osu!\n"
     ]
    }
   ],
   "source": [
    "from models.vae.vae_t import OsuReplayTVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f15e4c0-8409-4269-bf27-3240fa098a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplayTVAE initialized on cuda\n",
      "decoder parameters: 2806210\n",
      "encoder parameters: 2621952\n",
      "Total parameters: 5428162\n",
      "OsuReplayTVAE loaded from replaytvae_morerecent.pt\n"
     ]
    }
   ],
   "source": [
    "model = OsuReplayTVAE.load(\"replaytvae_morerecent.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "239efc00-d706-4004-9073-321cc12b3311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "import osu.dataset as dataset\n",
    "\n",
    "from models.annealer import Annealer\n",
    "from models.base import OsuModel\n",
    "from models.model_utils import TransformerArgs\n",
    "from models.vae.encoder_t import ReplayEncoderT\n",
    "\n",
    "\n",
    "\n",
    "class ReplayDecoder_Lstm(nn.Module):\n",
    "    def __init__(self, input_size, latent_dim=32, past_frames=0, future_frames=0):\n",
    "        super().__init__()\n",
    "        self.past_frames = past_frames\n",
    "        self.future_frames = future_frames\n",
    "        self.window_size = past_frames + 1 + future_frames\n",
    "\n",
    "        combined_size = (input_size) + latent_dim\n",
    "\n",
    "        # Symmetric layers to encoder (not really almost)\n",
    "        self.lstm = nn.LSTM(combined_size, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
    "\n",
    "        self.dense1 = nn.Linear(256, 128)\n",
    "        self.dense2 = nn.Linear(128, 64)\n",
    "\n",
    "        self.output_layer = nn.Linear(64, 2)  # x, y positions\n",
    "\n",
    "    def forward(self, beatmap_features, latent_code):\n",
    "        batch_size, seq_len, _ = beatmap_features.shape\n",
    "\n",
    "        # beatmap_features is already windowed\n",
    "        # Expand latent code to sequence length\n",
    "        latent_expanded = latent_code.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "\n",
    "        # Combine windowed features with latent code\n",
    "        x = torch.cat([beatmap_features, latent_expanded], dim=-1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        features = F.relu(self.dense1(lstm_out))\n",
    "        features = F.relu(self.dense2(features))\n",
    "\n",
    "        positions = self.output_layer(features)\n",
    "\n",
    "        return positions\n",
    "\n",
    "\n",
    "# switch from the RNN based VAE to a transformer based one\n",
    "# TODO! save hyperparam dict\n",
    "class OsuReplayTVAE_Lstm(OsuModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        annealer: Annealer = None,\n",
    "        batch_size=64,\n",
    "        device=None,\n",
    "        latent_dim=64,\n",
    "        transformer_args: TransformerArgs = None,\n",
    "        noise_std=0.0, \n",
    "        frame_window=(40, 90),\n",
    "        compile: bool = True\n",
    "    ):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.transformer_args = transformer_args or TransformerArgs()\n",
    "        self.past_frames = frame_window[0]\n",
    "        self.future_frames = frame_window[1]\n",
    "        self.noise_std = noise_std\n",
    "        self.annealer = annealer or Annealer(\n",
    "            total_steps=10, range=(0, 0.3), cyclical=True, stay_max_steps=5\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            batch_size=batch_size,\n",
    "            device=device,\n",
    "            compile=compile\n",
    "        )\n",
    "\n",
    "    def _initialize_models(self, **kwargs):\n",
    "        self.encoder = ReplayEncoderT(\n",
    "            input_size=self.input_size,\n",
    "            latent_dim=self.latent_dim,\n",
    "            transformer_args=self.transformer_args,\n",
    "            noise_std=self.noise_std,\n",
    "            past_frames=self.past_frames,\n",
    "            future_frames=self.future_frames,\n",
    "        )\n",
    "        self.decoder = ReplayDecoder_Lstm(\n",
    "            input_size=self.transformer_args.embed_dim,\n",
    "            latent_dim=self.latent_dim,\n",
    "            past_frames=self.past_frames,\n",
    "            future_frames=self.future_frames,\n",
    "        )\n",
    "\n",
    "    def _initialize_optimizers(self):\n",
    "        params = list(self.encoder.parameters()) + list(self.decoder.parameters())\n",
    "        self.optimizer = optim.AdamW(\n",
    "            params, lr=0.001, betas=(0.9, 0.999), weight_decay=0.001\n",
    "        )\n",
    "\n",
    "\n",
    "    def _train_epoch(self, epoch, total_epochs, **kwargs):\n",
    "        epoch_total_loss = 0\n",
    "        epoch_recon_loss = 0\n",
    "        epoch_kl_loss = 0\n",
    "\n",
    "        for i, (batch_x, batch_y_pos) in enumerate(\n",
    "            tqdm.tqdm(\n",
    "                self.train_loader,\n",
    "                disable=True,\n",
    "                position=1,\n",
    "                desc=f\"Epoch {epoch + 1}/{total_epochs} (Beta: {self.annealer.current()})\",\n",
    "            )\n",
    "        ):\n",
    "            self._set_custom_train_status(f\"Batch {i}/{len(self.train_loader)}\")\n",
    "            batch_x = batch_x.to(self.device)             # (B, T, features)\n",
    "            batch_y_pos = batch_y_pos.to(self.device)     # (B, T, pos)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            reconstructed, mu, logvar = self.forward(batch_x, batch_y_pos)\n",
    "\n",
    "            # Compute loss\n",
    "            total_loss, recon_loss, kl_loss = self.loss_function(\n",
    "                reconstructed, batch_y_pos, mu, logvar\n",
    "            )\n",
    "\n",
    "            # Backward pass\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(self.encoder.parameters()) + list(self.decoder.parameters()),\n",
    "                max_norm=1.0,\n",
    "            )\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_total_loss += total_loss.item()\n",
    "            epoch_recon_loss += recon_loss.item()\n",
    "            epoch_kl_loss += kl_loss.item()\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_total_loss = epoch_total_loss / len(self.train_loader)\n",
    "        avg_recon_loss = epoch_recon_loss / len(self.train_loader)\n",
    "        avg_kl_loss = epoch_kl_loss / len(self.train_loader)\n",
    "\n",
    "        # Step the annealer\n",
    "        self.annealer.step()\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": avg_total_loss,\n",
    "            \"recon_loss\": avg_recon_loss,\n",
    "            \"kl_loss\": avg_kl_loss,\n",
    "        }\n",
    "\n",
    "    # allow backpropogatino through sampling\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, beatmap_features, positions):\n",
    "        embeddings, mu, logvar = self.encoder(beatmap_features)\n",
    "\n",
    "        # Sample latent code\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        reconstructed = self.decoder(embeddings, z)\n",
    "\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "    # recon + kl term\n",
    "    def loss_function(self, reconstructed, original, mu, logvar):\n",
    "        # TODO! am i supposed to avg this? probably?\n",
    "        recon_loss = F.mse_loss(reconstructed, original, reduction=\"sum\")\n",
    "        recon_loss /= original.shape[0]\n",
    "\n",
    "        kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kld /= original.shape[0]\n",
    "\n",
    "        total_loss = recon_loss + self.annealer(kld)\n",
    "\n",
    "        return total_loss, recon_loss, kld\n",
    "\n",
    "    def _get_state_dict(self):\n",
    "        return {\n",
    "            \"encoder\": self.encoder.state_dict(),\n",
    "            \"decoder\": self.decoder.state_dict(),\n",
    "            \"latent_dim\": self.latent_dim,\n",
    "            \"transformer_args\": self.transformer_args.to_dict(),\n",
    "            \"noise_std\": self.noise_std,\n",
    "            \"input_size\": self.input_size,\n",
    "            \"past_frames\": self.past_frames,\n",
    "            \"future_frames\": self.future_frames,\n",
    "        }\n",
    "\n",
    "    def _load_state_dict(self, checkpoint):\n",
    "        self.encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "        self.decoder.load_state_dict(checkpoint[\"decoder\"])\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str, device: Optional[torch.device] = None, **kwargs):\n",
    "        device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        \n",
    "        # Load transformer configuration from checkpoint\n",
    "        transformer_args = TransformerArgs.from_dict(checkpoint[\"transformer_args\"])\n",
    "        \n",
    "        vae_args = {\n",
    "            \"latent_dim\": checkpoint.get(\"latent_dim\", 64),\n",
    "            \"transformer_args\": transformer_args,\n",
    "            \"noise_std\": checkpoint.get(\"noise_std\", 0.0),\n",
    "            \"frame_window\": (checkpoint.get(\"past_frames\", 40), checkpoint.get(\"future_frames\", 90)),\n",
    "        }\n",
    "\n",
    "        instance = cls(device=device, **kwargs, **vae_args)\n",
    "\n",
    "        instance._load_state_dict(checkpoint)\n",
    "        instance._set_eval_mode()\n",
    "\n",
    "        print(f\"{cls.__name__} loaded from {path}\")\n",
    "        return instance\n",
    "\n",
    "    def generate(self, beatmap_data, num_samples=1):\n",
    "        self._set_eval_mode()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            beatmap_tensor = torch.FloatTensor(beatmap_data).to(self.device)\n",
    "\n",
    "            batch_size = beatmap_tensor.shape[0]\n",
    "\n",
    "            # sample from prior distribution\n",
    "            z = torch.randn(batch_size, self.latent_dim, device=self.device)\n",
    "            embeddings, mu, logvar = self.encoder(beatmap_tensor)\n",
    "            # z = self.reparameterize(mu, logvar)\n",
    "\n",
    "            pos = self.decoder(embeddings, z)\n",
    "\n",
    "        self._set_train_mode()\n",
    "\n",
    "        return pos.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b91adb4-d92f-4294-b083-e0b87050587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplayVAE initialized on cuda\n",
      "decoder parameters: 452978\n",
      "encoder parameters: 646912\n",
      "Total parameters: 1099890\n",
      "OsuReplayVAE loaded from replayvae_most_recent.pt\n"
     ]
    }
   ],
   "source": [
    "# model = OsuReplayTVAE_Lstm.load(\"replaytvae_lstm_most_recent.pt\")\n",
    "from models.vae.vae import OsuReplayVAE\n",
    "model = OsuReplayVAE.load(\"replayvae_most_recent.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5583978-5d5b-4fd7-92b4-b4036b552bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2048, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test training results\n",
    "from osu.rulesets.mods import Mods\n",
    "import osu.rulesets.beatmap as bm\n",
    "import osu.dataset as dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "test_name = '1hope'\n",
    "test_mods = Mods.NONE\n",
    "test_map_path = f'assets/{test_name}_map.osu'\n",
    "test_song = f'assets/{test_name}_song.mp3'\n",
    "\n",
    "test_map = bm.load(test_map_path)\n",
    "test_map.apply_mods(test_mods)\n",
    "\n",
    "data = dataset.input_data(test_map)\n",
    "data = np.reshape(data.values, (-1, dataset.BATCH_LENGTH, len(dataset.INPUT_FEATURES)))\n",
    "data = torch.FloatTensor(data)#[:1, :, :]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddd772b4-96f4-4342-8a73-cf987d3cffdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated replay data shape: (10240, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03608687,  0.04201587,  0.        ,  0.        ],\n",
       "       [-0.06219175,  0.03570696,  0.        ,  0.        ],\n",
       "       [-0.08341248,  0.04224943,  0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.07666025,  0.09332845,  0.        ,  0.        ],\n",
       "       [-0.07671017,  0.09339259,  0.        ,  0.        ],\n",
       "       [-0.0767544 ,  0.09344955,  0.        ,  0.        ]],\n",
       "      shape=(10240, 4), dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_data = model.generate(data)\n",
    "\n",
    "import os\n",
    "\n",
    "replay_data = np.concatenate(replay_data)\n",
    "replay_data = np.pad(replay_data, ((0, 0), (0, 2)), mode='constant', constant_values=0)\n",
    "if not os.path.exists('.generated'):\n",
    "    os.makedirs('.generated')\n",
    "\n",
    "print(f\"Generated replay data shape: {replay_data.shape}\")\n",
    "\n",
    "replay_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0003d66-21df-4c3c-9166-1d101732f31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm.c:8772:(snd_pcm_recover) underrun occurred\n"
     ]
    }
   ],
   "source": [
    "import osu.preview.preview as preview\n",
    "\n",
    "preview.preview_replay_raw(replay_data, test_map_path, test_mods, test_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476a43a-93fa-40e8-bb76-ae62b25c794c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204bc08-ca13-4365-b67c-7862cc8fa751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
