# Notes
Notes for stuff related to this project, mostly for myself

- Use *sliding window* attention instead of full bidirectional attention and/or windowed feature input. Seems a lot better for local attended features, which is the goal?

# god tier papers
- Sliding window attention: https://arxiv.org/abs/2502.18845
- Diffusion model trajectory generation: https://arxiv.org/pdf/2402.07369
- Diffusion model for time series/spatio-temporal data: https://arxiv.org/pdf/2404.18886
- Transformer time series *forcasting* (not generation tho): https://www.sciencedirect.com/science/article/pii/S2665963824001040
- WGAN-GP: https://arxiv.org/pdf/1704.00028
- Attention is all you need: https://arxiv.org/abs/2502.18845

# other resources
- Sliding window transformer: https://www.youtube.com/watch?v=it0iZ93aLs4
